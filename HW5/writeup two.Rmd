---
title: "writeup twp"
author: "Tian Tan"
date: "April 16, 2016"
output: pdf_document
---

1. Likelihood and log likelihood functions
\begin{eqnarray*}
L(\beta_0,\beta_1) &=&  f(\pi_1,\pi_2,...\pi_n|(\beta_0,\beta_1)) \\
         &=&  \prod \pi_i^{y_i} (1-\pi_i)^{1-yi} \\
         &=&  \prod (\frac{e^{\beta_0+\beta_1 x_i}}{1+e^{\beta_0+\beta_1 x_i}})^{y_i} (1-\frac{e^{\beta_0+\beta_1 x_i}}{1+e^{\beta_0+\beta_1 x_i}})^{1-yi} \\
         &=&  \prod (e^{\beta_0+\beta_1 x_i}y_i) \cdot \frac{1}{1+e^{\beta_0+\beta_1 x_i}} \\
         &=&  e^{ \beta_0(\sum y_i)+\beta_1(\sum x_iy_i)} \cdot \frac{1}{1+e^{\beta_0+\beta_1 x_i}} \cdot 1
\end{eqnarray*}
Since $\frac{1}{1+e^{\beta_0+\beta_1 x_i}}$ is known, according to the Factorization Theorem, and $h(y_1,y_2,...y_n)=1$, we have $\sum y_i$ and $\sum x_iy_i$ as sufficient statistics

\begin{eqnarray*}
l(\beta_0,\beta_1) &=&  \sum (y_i log(\pi_i)+(1-y_i)log(1-\pi_i))\\
     &=&  \sum (y_i(\beta_0+\beta_1 x_i)-y_i log(1+e^{\beta_0+\beta_1 x_i})-(1-y_i)log(1+e^{\beta_0+\beta_1 x_i}))\\
     &=&  \sum (y_i(\beta_0+\beta_1 x_i)-log(1+e^{\beta_0+\beta_1 x_i}))
\end{eqnarray*}

2.Coordinate ascent and golden section code from Lecture 10 to determine the ML estimates of the regression coefficients
```{r}
oringCA<-function(filename){
  data = read.csv(filename)
  x=data[,3]  #vector of x
  y=data[,2]>0 #vector of y
  n=length(data[,1])
  ybar=sum(y)/n
  b0=log(ybar/(1-ybar))
  b1=0
  mu=c(b0,b1)
  
  GoldenSection = function(fn,mul,mur,mu,dim,X,Y,tol=1e-8,maxit=10000){
    # Here mu is a vector of inputs into fn, mul and mur are upper and lower
    # values for mu, also given as vectors. However, the function only works
    # on the dimension dim.
    
    gr = (1 + sqrt(5))/2  # Pre-calculate golden ratio
    xl = mu               # We'll set xl, xr and xm to be vectors, but only
    xl[dim] = mul[dim]    # the dim'th entry will be different. Note that in
                          # all our updating rules, the other entries will
    xr = mu               # not be affected.
    xr[dim] = mur[dim]
    
    xm = mu
    xm[dim] =  mul[dim] + (mur[dim]-mul[dim])/(1+gr)
    
    fl = fn(xl,X,Y); fr = fn(xr,X,Y); fm = fn(xm,X,Y)
    
    tol.met = FALSE    # No tolerance met
    iter = 0           # No iterations
    
    while(!tol.met){         # Here we only need to check conditions on the
      iter = iter + 1        # dim'th entry.
      
      if( (xr[dim]-xm[dim]) > (xm[dim]-xl[dim]) ){  # Working on the right-hand side
        y = xm + (xr-xm)/(1+gr); fy = fn(y,X,Y);
        if( fy > fm){ xl = xm; fl = fm; xm = y; fm = fy }
        else{ xr = y; fr = fy }
      }
      else{
        y = xm - (xm-xl)/(1+gr); fy = fn(y,X,Y);
        if( fy > fm){ xr = xm; fr = fm; xm = y; fm = fy }
        else{ xl = y; fl = fy }
      }
      
      if( (xr[dim]-xm[dim]) < tol | iter > maxit ){ tol.met=TRUE }
    }
    return(list(xm=xm,iter=iter))
  }
  
  CoordinateAscent = function(mu,mul,mur,fn,X,Y,tol=1e-8,maxit=10000)
  {
    iter = 0              # Initialization
    tol.met = FALSE
    muhist = c()
    while(!tol.met){  # Tolerance will be checked by how much we move mu
      oldmu = mu      # over one cycle accross the dimensions.
      
      for(dim in 1:length(mu)){    # But we'll update the history at each
        iter = iter + 1            # coordinate
        mu = GoldenSection(fn,mul,mur,mu,dim,X,Y)$xm
        muhist = rbind(muhist,mu)
      }
      
      if( max(abs(mu - oldmu))< tol | iter > maxit){
        tol.met = TRUE
      } else{
        oldmu = mu 
      }
    }
    
    return(list(mu=mu, iter=iter, muhist = muhist))
  }
  
  result=CoordinateAscent(mu,c(-100,-100),c(100,100),loglikelihood,x,y,tol=1e-8,maxit=10000)
  mu_final=result$mu
  b0=mu_final[1]
  b1=mu_final[2]
  num_iter=result$iter
  return(c(num_iter,b0,b1))
}

loglikelihood<-function(mu,x,y){
  l= sum(y*(mu[1]+mu[2]*x)-log(1+exp(mu[1]+mu[2]*x)))
  return (l)
}

```

3. Formulas for the gradient vector and the Hessian matrix

Formula for the gradient vector
\begin{eqnarray*}
\Delta f(\beta) &=&(df/d\beta_0,df/d\beta_1) \\
df/d\beta_0 &=& \sum\nolimits_{i} y-\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} \\
df/d\beta_1 &=& \sum\nolimits_{i} xy-\frac{xe^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}
\end{eqnarray*}

Formula for the Hessian matrix
\begin{gather*}
H*(\beta) = \begin{pmatrix} d^2f(\beta)/d\beta_0 d\beta_0 & d^2(f\beta)/d\beta_0 d\beta_1 \\
               d^2f(\beta)/d\beta_1 d\beta_0 & d^2f(\beta)/d\beta_1 d\beta_1 \end{pmatrix}\quad
\end{gather*}
\begin{eqnarray*}
d^2f(\beta)/d\beta_0 d\beta_0 &=& \sum\nolimits_{i} \frac{-e^{\beta_0+\beta_1x}}{(1+e^{\beta_0+\beta_1x})^2} \\
d^2(f\beta)/d\beta_0 d\beta_1 &=& \sum\nolimits_{i} \frac{-xe^{\beta_0+\beta_1x}}{(1+e^{\beta_0+\beta_1x})^2}\\
d^2f(\beta)/d\beta_1 d\beta_0 &=& \sum\nolimits_{i} \frac{-xe^{\beta_0+\beta_1x}}{(1+e^{\beta_0+\beta_1x})^2}\\
d^2f(\beta)/d\beta_1 d\beta_1 &=& \sum\nolimits_{i} \frac{-x^2e^{\beta_0+\beta_1x}}{(1+e^{\beta_0+\beta_1x})^2}
\end{eqnarray*}



4. Newton-Raphson algorithm to determine the maximum likelihood estimates of the regression coefficients
```{r}
oringNR<-function(filename){
  data = read.csv(filename)
  x=data[,3]  #vector of x
  y=data[,2]>0  #vector of y
  n=length(data[,1])
  ybar=sum(y)/n
  b0=log(ybar/(1-ybar))
  b1=0
  mu=c(b0,b1)
  
  NewtonRaphson2 = function(mu,dfn,d2fn,X,Y,tol=1e-8,maxit=100){
    tol.met=FALSE; iter = 0; iterhist = mu
    result=list()
    while(!tol.met){
      result[iter]=list(c(iter,mu))
      iter = iter + 1
      oldmu = mu
      g = dfn(mu,X,Y)            # Gradient
      H = d2fn(mu,X,Y)           # Hessian
      mu = mu - solve(H,g)     # Update
      iterhist = rbind(iterhist,mu)
      
      if( (max(abs( mu-oldmu )) < tol & max(abs(g)) < tol) | iter > maxit){
        tol.met=TRUE 
      }
    }
    return(result)
  }
  
  result=NewtonRaphson2(mu,dfn,d2fn,x,y,tol=1e-8,maxit=100)
  result=data.frame(result)
  histOR=t(result)
  iter=length(histOR[,1])
  colnames(histOR)=c("iter","beta0","beta1")
  rownames(histOR)=c(1:iter)
  return(histOR)
}

dfn = function(mu,x,y){
  l= sum(y*(mu[1]+mu[2]*x)-log(1+exp(mu[1]+mu[2]*x)))
  dx1 = sum(y-exp(mu[1]+mu[2]*x)/(1+exp(mu[1]+mu[2]*x)))
  dx2 = sum(y*x-x*exp(mu[1]+mu[2]*x)/(1+exp(mu[1]+mu[2]*x)))
  return(c(dx1,dx2))
}

d2fn = function(mu,x,y){
  dx11 = sum(-exp(mu[1]+mu[2]*x)/(1+exp(mu[1]+mu[2]*x))^2)  # Second derivatives
  dx12 = sum(-x*exp(mu[1]+mu[2]*x)/(1+exp(mu[1]+mu[2]*x))^2)
  dx21 = sum(-x*exp(mu[1]+mu[2]*x)/(1+exp(mu[1]+mu[2]*x))^2)
  dx22 = sum(-x^2*exp(mu[1]+mu[2]*x)/(1+exp(mu[1]+mu[2]*x))^2)
  return(matrix( c(dx11,dx12,dx21,dx22),2,2))
}
```

5.Construct a plot of the probability of o-ring damage as a function of temperature.
```{r}
result=oringNR("shuttle.csv")
file=read.csv("shuttle.csv")
n=length(result[,1])
temp=c(30:80)
plot(temp,exp(result[n,2]+result[n,3]*temp)/(1+exp(result[n,2]+result[n,3]*temp)),main="probability of o-ring damages versus temperature",ylim=c(0,1),xlab="temperature",ylab="probability of o-ring damages",type="l")
abline(h=1)
prob=exp(result[n,2]+result[n,3]*32)/(1+exp(result[n,2]+result[n,3]*32))
prob
```
So the predicted probability of o-ring damage when temperature at launch is 32F is 0.9995066

6. Fit the logistic regression model using the glm function in R and compare the estimates to those obtained using the CA and NR methods
```{r}
file$ndo=file$ndo>0
oring.glm=glm(ndo~temp,family=binomial,data=file)
summary(oring.glm)
```
The intersect is beta0 15.0429 and the temp coefficient is beta1 -0.2322. They are very close to the result of oringCA(beta0 is 14.0428333, beta1 is -0.2321615) and are exactly the same as the result of oringNR.

7.Construct a histogram of the permutation distribution 
```{r}
l=length(file[,2])
sumresult=list()
m=0
for (i in 1:10000){
  permute=sample(1:l)
  y_vec=file[,2][permute]
  sum_xy=sum(y_vec*file[,3])
  sumresult[i]=sum_xy
  if (sum_xy<=sum(file$ndo*file$temp)){
    m=m+1
  }
}
x=unlist(sumresult)
hist(x,main="permutation distribution of sum xy",freq=FALSE,xlab="sum of xy")
abline(v=sum(file$ndo*file$temp),col="red")
text(sum(file$ndo*file$temp)+10,0.02,"observed",col="red")
prob=m/10000
prob
```
The probability of permutation values that are less than or equal to the observed value is 0.005. 

8.Determine the exact permutation probability of a value less than or equal to the observed.

```{r}
id=seq(1:l)
comb=combn(id,sum(file[,2]))
k=0
for (i in 1:length(comb[1,])){
    idx=comb[,i]
    y_vec[idx]=1
    new_sum=sum(y_vec*file[,3])
    if (new_sum<=sum(file[,3]*file[,2])){
      k=k+1
    }
    y_vec=rep(0,23)
}
prob2=k/length(comb[1,])
prob2
```
Determine the exact permutation probability of a value less than or equal to the observed is 0.004491



