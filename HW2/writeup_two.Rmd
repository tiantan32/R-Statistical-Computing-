---
title: "writeup_two"
author: "Tian Tan"
date: "February 20, 2016"
output: pdf_document
---

Part Two: Investigative Simulations

1. Three Notions of Distance 

1) expression for $v_1^2$ and $v_2^2$
\begin{eqnarray*}
v_1^2 &=& <\mathbf{x},\mathbf{r}>^2 \\
      &=& (\sum_{[i=1}^p x_ir_i)^2
\end{eqnarray*}  

\begin{eqnarray*}
v_2^2 &=& <\mathbf{y},\mathbf{r}>^2 \\
      &=& (\sum_{[i=1}^p y_ir_i)^2
\end{eqnarray*}  

2) expression for the expection $E[v_1^2]$ and $E[v_2^2]$
\begin{eqnarray*}
E[v_1^2] &=& \sum_{i=1}^p (x_i)^2E(r_i^2)+\sum_{i\not=j}x_ix_jE(r_ir_j) \\
         &=& \sum_{i=1}^p x_i^2+ \sum_{i\not=j} E(r_ir_j)
\end{eqnarray*}         

Since $r_is$ are identical independently distributed, when $i\not=j$:
\begin{eqnarray*}
E(r_ir_j) &=& E(r_i)E(r_j)=0
\end{eqnarray*}
So: 
\begin{eqnarray*}
E(v_1^2) &=& \sum_{i=1}^p x_i^2
\end{eqnarray*}  

Following the same process, we have
\begin{eqnarray*}
E[v_2^2] &=& \sum_{i=1}^p y_i^2
\end{eqnarray*} 

3) expression for $E((v_1-v_2)^2)$ and $E[v_1v_2]$
\begin{eqnarray*}
E((v_1-v_2)^2) &=& E((\sum_{i=1}^p (x_i-y_i)r_i)^2) \\
               &=& E(\sum_{i=1}^p (x_i-y_i)^2)E((r_i)^2) \\
               &=& \sum_{i=1}^p (x_i-y_i)^2
\end{eqnarray*}

\begin{eqnarray*}
E[v_1v_2] &=& E[(x_1r_1+x_2r_2+...+x_pr_p)(y_1r_1+y_2r_2+...+x_pr_p)] \\
          &=& E[\sum_{i=1}^p x_iy_iE(r_i^2)+\sum_{i\not=j} x_iy_jr_ir_j] \\
          &=& \sum_{i=1}^p x_iy_iE(r_i^2)+\sum_{i\not=j} x_iy_jE(r_ir_j) \\
          &=& \sum_{i=1}^p x_iy_i
\end{eqnarray*}

4) pattern
\begin{eqnarray*}
E((v_1-v_2)^2) &=& \sum_{i=1}^p (x_i-y_i)^2
\end{eqnarray*}
is the Euclidean distance between the two p dimensional vectors
\begin{eqnarray*}
E(v_1^2) &=& \sum_{i=1}^p x_i^2
\end{eqnarray*}
is the length of $\mathbf{x}$ vector
\begin{eqnarray*}
E[v_1v_2] &=& <x,y> = \sum_{i=1}^p x_iy_i
\end{eqnarray*}
is the inner product of $\mathbf{x}$ and $\mathbf{y}$

5) explanation

For the norm of $x_i$, the estimation of the sum of each entry square on the ith row of V is the norm square of $x_i$;

For the Euclidean distance between $x_i$ and $x_j$, it is the expectation of sum of $(V_i-V_j)$ square;

For the inner product of $x_j$ and $x_i$, it is the estimation of product of $V_i$ and $V_j$.


2. Three Probability Distributions 

For $s\geq1$

The mean of Sparse Bernoulli:

$u=\sqrt(s)*\frac{1}{2s}-\sqrt(s)*\frac{1}{2s}+0*(1-\frac{1}{s})=0$

The second moment of Sparse Bernoulli: 

$u_2=(\sqrt(s))^2*\frac{1}{2s}+(-\sqrt(s))^2*\frac{1}{2s}+0*(1-\frac{1}{s})=1$



3. Simulation Process
```{r}
n = 1000
set.seed(1)
X1 = matrix(rnorm(n*100), nrow = n)
X2 = matrix(rnorm(n*1000), nrow = n)
X3 = matrix(rnorm(n*10000), nrow = n)

```

4.Computation versus Theory

The second line might be better than the first line. Because The first line needs further implementation to simplify the result, while the second line of code is much easier to execute, which may help decrease rounding errors. 

5. Speed of random projections 

helper function: gen_SB
```{r}
gen_SB<-function(p,k,s){
# generate R from SB(s) 
  if (s==1){
    r=runif(p*k,-1,1)
    R=matrix(abs(r)/r,p,k)
  }
  if (s>1){
    r=1*sqrt(s)
#  r should be related to uniform distribution   
    r=r*trunc(runif(p*k,-1-1/(s-1),1+1/(s-1)))
    R=matrix(r,p,k)
  }
  return (R)
}
```

time taken

```{r}
speed<-function(X,t){
  library(Matrix)
  time=0
  time=matrix(rep(0,36),nrow=9)
  vec=c(10,20,30,40,50,100,250,500,1000)
  for (i in c(1:t)){
    for (m in c(1:9)){
      k=vec[m]
      p=dim(X)[2]
      Rnorm=matrix(rnorm(p*k),nrow=p)
      Rrademacher=Matrix(1/sqrt(1)*gen_SB(p,k,1))
      RSB20=Matrix(1/sqrt(20)*gen_SB(p,k,20))
      tic=proc.time()
      VtVnorm= 1/k *(X %*% Rnorm) %*% t(X %*% Rnorm)
      toc = proc.time() - tic
      time[m,1]=time[m,1]+toc[3]
      tic=proc.time()
      VtVrademacher = 1/k *(X %*% Rrademacher) %*% t(X %*% Rrademacher)
      toc = proc.time() - tic
      time[m,2]=time[m,2]+toc[3]
      tic=proc.time()
      VtVsb20= 1/k *(X %*% RSB20) %*% t(X %*% RSB20)
      toc = proc.time() - tic
      time[m,3]=time[m,3]+toc[3]
      tic=proc.time()
      XtX = X %*% t(X) 
      toc = proc.time() - tic
      time[m,4]=time[m,4]+toc[3]
     }
 } 
 time=time/t
 return (time)
}

vec=c(10,20,30,40,50,100,250,500,1000)
result1=speed(X1,10)
plot(vec,result1[,1],main="X1 Time compareation", xlab="k",ylab="time",col="RED",type="l")
lines(vec,result1[,2],col="BLUE")
lines(vec,result1[,3],col="YELLOW")
lines(vec,result1[,4],col="GREEN")
legend("topleft",c("Rnorm","Rrademacher","Rsb20","XtX"),
       lty=c(1,1),lwd=c(2.5,2.5),col=c("RED","BLUE","YELLOW","GREEN"))

result2=speed(X2,10)
plot(vec,result2[,1],main="X2 Time compareation", xlab="k",ylab="time",col="RED",type="l")
lines(vec,result2[,2],col="BLUE")
lines(vec,result2[,3],col="YELLOW")
lines(vec,result2[,4],col="GREEN")
legend("topleft",c("Rnorm","Rrademacher","Rsb20","XtX"),
       lty=c(1,1),lwd=c(2.5,2.5),col=c("RED","BLUE","YELLOW","GREEN"))

result3=speed(X3,10)
plot(vec,result3[,1],main="X3 Time compareation", xlab="k",ylab="time",col="RED",type="l")
lines(vec,result3[,2],col="BLUE")
lines(vec,result3[,3],col="YELLOW")
lines(vec,result3[,4],col="GREEN")
legend("topleft",c("Rnorm","Rrademacher","Rsb20","XtX"),
       lty=c(1,1),lwd=c(2.5,2.5),col=c("RED","BLUE","YELLOW","GREEN"))
```
Comment: The three lines of X are linear and close to each other when the size of X is small, which means the larger amount of k the longer time required. Since XtX is not related with k, the time program runs are not related to the amount of k, so the line of XtX is a flat line. 
Moreover, as the size of X increase, the shape of lines from norm and rademacher method remains the same but the y-axis value goes up, while the line of sparse bernoulli method becomes flatter. The time required for XtX also increases, but its line is still flat. 



6. Accuracy of random projections

1) Find the actual value of the Euclidean distance between the first two rows of X1, X2 and X3
```{r}
accuracy<-function(X,t){
   library(Matrix)
   vec2=c(10,20,30,40,50,100,250,500,600,700,800,900,1000)
   p=dim(X)[2]
   distance= sqrt(sum((X[1,]-X[2,])^2))
   MSEs=matrix(0,nrow=length(vec2),ncol=3)
   
 for (time in c(1:t)){
     for (i in c(1:13)){
        k=vec2[i]
        Rnorm=matrix(rnorm(p*k),nrow=p)
        Rrademacher=Matrix(gen_SB(p,k,1))
        RSB20=Matrix(gen_SB(p,k,20))  
        
        Vnorm=(1/sqrt(k)) *X[1:2,] %*% Rnorm 
        Vrademacher=(1/sqrt(k)) *X[1:2,] %*% Rrademacher
        Vsb20=(1/sqrt(k)) *X[1:2,] %*% RSB20
        
        ED=sqrt(sum((Vnorm[1,]-Vnorm[2,])^2))
        MSEs[i,1]=MSEs[i,1]+(ED-distance)^2
        
        ED2=sqrt(sum((Vrademacher[1,]-Vrademacher[2,])^2))
        MSEs[i,2]=MSEs[i,2]+(ED2-distance)^2
        
        ED3=sqrt(sum((Vsb20[1,]-Vsb20[2,])^2))
        MSEs[i,3]=MSEs[i,3]+(ED3-distance)^2
   }
 }
 MSEs=MSEs/t
 relativeMSE=MSEs/distance
 return (relativeMSE)
}

vec2=c(10,20,30,40,50,100,250,500,600,700,800,900,1000)

#for x1
result1=accuracy(X1,100)
plot(vec2,result1[,1],xlab="k",ylab="MSE",main="X1 MSE comparision",col="RED",type="l")
lines(vec2,result1[,2],col="BLUE")
lines(vec2,result1[,3],col="GREEN")
legend("topright",c("Rnorm","Rrademacher","Rsb20"),
       lty=c(1,1),lwd=c(2.5,2.5),col=c("RED","BLUE","GREEN"))

#for X2
result2=accuracy(X2,100)
plot(vec2,result2[,1],xlab="k",ylab="MSE",main="X2 MSE comparision",col="Red",type="l")
lines(vec2,result2[,2],col="BLUE")
lines(vec2,result2[,3],col="GREEN")
legend("topright",c("Rnorm","Rrademacher","Rsb20"),
       lty=c(1,1),lwd=c(2.5,2.5),col=c("RED","BLUE","GREEN"))

#for X3
result3=accuracy(X3,100)
plot(vec2,result3[,1],xlab="k",ylab="MSE",main="X3 MSE comparision",col="Red",type="l")
lines(vec2,result3[,2],col="BLUE")
lines(vec2,result3[,3],col="GREEN")
legend("topright",c("Rnorm","Rrademacher","Rsb20"),lty=c(1,1),
       lwd=c(2.5,2.5),col=c("RED","BLUE","GREEN"))

```
Commentary: All three distributions seem to have the same logarithmically decreasing MSE pattern (drop almost vertically when k is small) and similar MSE values, so the three lines are mostly overlapped. Moreover, as the size of X increases, the lines become less smooth at the beginning. . 

Explanation: The MSE surely depends on the size of R matrix, i.e. the larger amount of k, the lower MSE, as is shown in the plot, which can be explained by the weak law of large number theory, i.e. the longer time repeated, the closer the ED to the actual ED, which means the smaller MSE. Secondly, since three lines almost overlap each other, it is hard to tell which distribution of $r_ij$ in R has the lowest MSE. But sparse bernoulli distributed R has relatively larger MSE when k is small, which makes sense, because sparse bernoulli distributed R has a lot of 0 in its matrix, which leads to smaller individual estimated Euclidean distances, and cause larger MSE.  

7. Using random projections for very large dataset

From the speed plots, we can see the time taken to run through computing process that calculates the pairwise Euclidean distances of $X_n*p. We can tell that if the size of dataset is the same, sparse bernoulli has the shortest time(the smallest slope), especially when the dimension of R becomes larger. 
As for accuracy, we can tell from the three plots that as the size of X increases, the MSE value becomes larger. However, all three distributions of R matrix have the same effect on estimating the actual Euclidean distances and calculating MSE. Which means we can chhoose from any of these three R distributions without changing the accuracy of the calculation.
So we can use the sparse bernoulli distributed R matrix to do random projections for very large datasets. In that case, we can have high accuracy as long as the dimension of X is very large, and meanwhile have relatively shorter time to run the function to find pairwise Euclidean distances. 
As for looking for the pairwise Euclidean distances, a quicker method is to look into $VV^T=(1/\sqrt(k)XR)(1/\sqrt(k)XR)^T$ equation from the computation versus theory part and found the Euclidean distance between $x_i$ and $x_j$ using $\sqrt(Vii^2+Vjj^2+2V_iV_j)$








